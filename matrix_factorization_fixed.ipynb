import numpy as np
import pandas as pd
import h5py
from scipy import stats
import pickle

# Gradient Descent with L2 Regularization
with h5py.File('rating_tr_numpy.h5', 'r') as hf:
    rating_tr = hf['rating'][:]
with h5py.File('rating_val_numpy.h5', 'r') as hf:
    rating_val = hf['rating'][:]

X_train = rating_tr[:, :-1]
y_train = rating_tr[:, -1]
X_val = rating_val[:, :-1]
y_val = rating_val[:, -1]

# Normalize the features
mean = np.mean(X_train, axis=0)
std = np.std(X_train, axis=0)
X_train = (X_train - mean) / std
X_val = (X_val - mean) / std

np.random.seed(42)
weights = np.random.randn(X_train.shape[1]) * 0.01
bias = 0.0
learning_rate = 0.001
lambda_reg = 0.1
num_epochs = 1000
prev_val_loss = float('inf')

for epoch in range(num_epochs):
    y_pred = np.dot(X_train, weights) + bias
    loss = np.mean((y_pred - y_train) ** 2) + lambda_reg * np.sum(weights ** 2)
    grad_weights = (2 / X_train.shape[0]) * np.dot(X_train.T, (y_pred - y_train)) + 2 * lambda_reg * weights
    grad_bias = (2 / X_train.shape[0]) * np.sum(y_pred - y_train)
    weights -= learning_rate * grad_weights
    bias -= learning_rate * grad_bias
    
    if epoch % 100 == 0:
        val_loss = np.mean((np.dot(X_val, weights) + bias - y_val) ** 2) + lambda_reg * np.sum(weights ** 2)
        print(f"Epoch {epoch}, Training Loss: {loss}, Validation Loss: {val_loss}")
        
        # Early stopping
        if abs(prev_val_loss - val_loss) < 1e-6:
            print("Early stopping triggered")
            break
        prev_val_loss = val_loss

y_val_pred = np.dot(X_val, weights) + bias
val_loss = np.mean((y_val_pred - y_val) ** 2) + lambda_reg * np.sum(weights ** 2)
print(f"Final Validation Loss: {val_loss}")
