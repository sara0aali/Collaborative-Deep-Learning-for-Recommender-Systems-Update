import numpy as np
import h5py
from sklearn.model_selection import train_test_split

# Load training data
with h5py.File('rating_tr_numpy.h5', 'r') as hf:
    rating_tr = hf['rating'][:]

# Load validation data
with h5py.File('rating_val_numpy.h5', 'r') as hf:
    rating_val = hf['rating'][:]

# Flatten the data for simplicity (assumes binary classification or regression)
X_train = rating_tr[:, :-1]  # Features
y_train = rating_tr[:, -1]   # Target

X_val = rating_val[:, :-1]
y_val = rating_val[:, -1]

# Initialize parameters
np.random.seed(42)
weights = np.random.randn(X_train.shape[1])
bias = 0.0
learning_rate = 0.01
lambda_reg = 0.1  # L2 regularization strength
num_epochs = 1000

# Gradient descent with L2 regularization
for epoch in range(num_epochs):
    # Predictions
    y_pred = np.dot(X_train, weights) + bias

    # Compute loss (Mean Squared Error with L2 regularization)
    loss = np.mean((y_pred - y_train) ** 2) + lambda_reg * np.sum(weights ** 2)

    # Gradients
    grad_weights = (2 / X_train.shape[0]) * np.dot(X_train.T, (y_pred - y_train)) + 2 * lambda_reg * weights
    grad_bias = (2 / X_train.shape[0]) * np.sum(y_pred - y_train)

    # Update weights and bias
    weights -= learning_rate * grad_weights
    bias -= learning_rate * grad_bias

    # Print loss every 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss}")

# Validation
y_val_pred = np.dot(X_val, weights) + bias
val_loss = np.mean((y_val_pred - y_val) ** 2) + lambda_reg * np.sum(weights ** 2)
print(f"Validation Loss: {val_loss}")
