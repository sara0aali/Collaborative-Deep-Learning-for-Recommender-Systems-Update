import numpy as np
import pandas as pd
import h5py
from scipy import stats
import pickle

# Read rating matrix
store = pd.HDFStore('ratingDF_tr.h5')
df_rating_tr = store['df_rating_tr']
store = pd.HDFStore('ratingDF_val.h5')
df_rating_val = store['df_rating_val']

user_id = np.array(df_rating_tr['ncodpers'].unique())
prod_id = np.array(df_rating_tr['prodIdx'].unique())

# Read the matrix form of the rating matrix
store = pd.HDFStore('rating_mat_tr.h5')
rating_tr = store['rating_mat']
store = pd.HDFStore('rating_mat_val.h5')
rating_val = store['rating_mat']

# Generate a small subset
rating_tr_short = rating_tr.head(50000)
rating_val_short = rating_val.head(50000)

# Store the subset into file
with h5py.File('rating_tr_numpy.h5', 'w') as hf:
    hf.create_dataset("rating", data=rating_tr_short.values)
with h5py.File('rating_val_numpy.h5', 'w') as hf:
    hf.create_dataset("rating", data=rating_val_short.values)

# Generate the encoding of user information
store = pd.HDFStore('autoenc_inp.h5')
df_autoenc = store['df_autoenc']
index = np.array(rating_tr_short.index)
df = df_autoenc.loc[df_autoenc['ncodpers'].isin(index)]
df = df.sort_values('ncodpers')
judge = (df['ncodpers'].values == index)
INPUT_LAYER = 314
xtrain = np.zeros((df.shape[0], INPUT_LAYER), dtype=np.int64)
for i in range(df.shape[0]):
    xtrain[i] = df.values[i][1]
with h5py.File('user_infor.h5', 'w') as hf:
    hf.create_dataset("infor", data=xtrain)

rating_val_new = rating_val.iloc[50001:100001]
rating_tr_new = rating_tr.iloc[50001:100001]
with h5py.File('rating_val_numpy_new.h5', 'w') as hf:
    hf.create_dataset("rating", data=rating_val_new.values)
with h5py.File('rating_tr_numpy_new.h5', 'w') as hf:
    hf.create_dataset("rating", data=rating_tr_new.values)

index_new = np.array(rating_tr.index)[50001:100001]
df_new = df_autoenc.loc[df_autoenc['ncodpers'].isin(index_new)]
df_new = df_new.sort_values('ncodpers')
x_new = np.zeros((df_new.shape[0], INPUT_LAYER), dtype=np.int64)
for i in range(df_new.shape[0]):
    x_new[i] = df_new.values[i][1]
with h5py.File('user_infor_new.h5', 'w') as hf:
    hf.create_dataset("infor", data=x_new)

# Gradient Descent with L2 Regularization
with h5py.File('rating_tr_numpy.h5', 'r') as hf:
    rating_tr = hf['rating'][:]
with h5py.File('rating_val_numpy.h5', 'r') as hf:
    rating_val = hf['rating'][:]

X_train = rating_tr[:, :-1]
y_train = rating_tr[:, -1]
X_val = rating_val[:, :-1]
y_val = rating_val[:, -1]

np.random.seed(42)
weights = np.random.randn(X_train.shape[1]) * 0.01
bias = 0.0
learning_rate = 0.001
lambda_reg = 0.1
num_epochs = 1000

for epoch in range(num_epochs):
    y_pred = np.dot(X_train, weights) + bias
    loss = np.mean((y_pred - y_train) ** 2) + lambda_reg * np.sum(weights ** 2)
    grad_weights = (2 / X_train.shape[0]) * np.dot(X_train.T, (y_pred - y_train)) + 2 * lambda_reg * weights
    grad_bias = (2 / X_train.shape[0]) * np.sum(y_pred - y_train)
    weights -= learning_rate * grad_weights
    bias -= learning_rate * grad_bias
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss}")

y_val_pred = np.dot(X_val, weights) + bias
val_loss = np.mean((y_val_pred - y_val) ** 2) + lambda_reg * np.sum(weights ** 2)
print(f"Validation Loss: {val_loss}")
